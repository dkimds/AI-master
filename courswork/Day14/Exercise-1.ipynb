{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 예시: [[ 4  4 10 10  5]\n",
      " [ 7  7  4  9  8]\n",
      " [ 4  3  5 10  9]\n",
      " [ 8  2  7  8  2]\n",
      " [ 3  7  2  3  1]]\n",
      "입력 데이터 예시: [[ 5 10 10  4  4]\n",
      " [ 8  9  4  7  7]\n",
      " [ 9 10  5  3  4]\n",
      " [ 2  8  7  2  8]\n",
      " [ 1  3  2  7  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤 데이터 생성\n",
    "def generate_data(num_samples=1000):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(num_samples):\n",
    "        sample = np.random.randint(1, 11, size=5)\n",
    "        X.append(sample)\n",
    "        y.append(sample[::-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_data()\n",
    "\n",
    "print(\"입력 데이터 예시:\", X[:5])\n",
    "print(\"입력 데이터 예시:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 30.3293   \n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0629 \n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2050\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1257 \n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8716 \n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6845 \n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8833 \n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0572\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4613 \n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9936 \n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6475 \n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4231 \n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3451 \n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2732 \n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2224 \n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1926\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1668 \n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1552 \n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1353\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x313924b90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    LSTM(100, input_shape=(5, 1)),\n",
    "    Dense(5, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 데이터 reshape\n",
    "X = X.reshape(-1, 5, 1)\n",
    "y = y.reshape(-1, 5)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X, y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345 거꾸로 출력: 54321\n",
      "98765 거꾸로 출력: 56799\n",
      "45678 거꾸로 출력: 87654\n"
     ]
    }
   ],
   "source": [
    "def reverse_input(input_str):\n",
    "    # 입력 문자열을 리스트로 변환\n",
    "    input_list = [int(x) for x in input_str]\n",
    "    # numpy 배열로 변환 후 모델 입력 형태로 reshape\n",
    "    input_array = np.array(input_list).reshape(1, 5, 1)\n",
    "    # 모델 예측\n",
    "    prediction = model.predict(input_array, verbose=0)\n",
    "    # 예측 결과를 정수로 변환하여 리스트로 변환\n",
    "    result = [int(round(x)) for x in prediction[0]]\n",
    "    return ''.join(map(str, result))\n",
    "\n",
    "# 입력 반복\n",
    "while True:\n",
    "    user_input = input(\"숫자 5개를 입력하세요 (Quit 입력 시 종료): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    elif len(user_input) == 5 and user_input.isdigit():\n",
    "        print(user_input, \"거꾸로 출력:\", reverse_input(user_input))\n",
    "    else:\n",
    "        print(\"잘못된 입력입니다. 숫자 5개를 입력하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 -> 모델 예측 -> 실제 출력\n",
      "73671 -> 17637 -> 17637\n",
      "495109 -> 99594 -> 910594\n",
      "791068 -> 861097 -> 861097\n",
      "561103 -> 310165 -> 310165\n",
      "241012 -> 21942 -> 211042\n",
      "82513 -> 31528 -> 31528\n",
      "89628 -> 82698 -> 82698\n",
      "283410 -> 104382 -> 104382\n",
      "6410510 -> 1051046 -> 1051046\n",
      "57365 -> 56285 -> 56375\n",
      "37171 -> 27173 -> 17173\n",
      "92442 -> 24429 -> 24429\n",
      "28574 -> 47591 -> 47582\n",
      "49211 -> 21294 -> 11294\n",
      "84941 -> 14948 -> 14948\n",
      "1093210 -> 92499 -> 1023910\n",
      "110356 -> 653102 -> 653101\n",
      "42413 -> 31424 -> 31424\n",
      "518210 -> 92715 -> 102815\n",
      "483105 -> 510384 -> 510384\n",
      "98793 -> 39789 -> 39789\n",
      "1077101 -> 19779 -> 1107710\n",
      "61714 -> 41716 -> 41716\n",
      "487610 -> 106784 -> 106784\n",
      "27591 -> 18572 -> 19572\n",
      "42389 -> 98324 -> 98324\n",
      "23798 -> 89732 -> 89732\n",
      "13464 -> 46431 -> 46431\n",
      "38672 -> 27682 -> 27683\n",
      "84357 -> 75349 -> 75348\n",
      "28394 -> 49382 -> 49382\n",
      "291043 -> 341092 -> 341092\n",
      "104592 -> 295410 -> 295410\n",
      "42554 -> 45524 -> 45524\n",
      "86544 -> 44569 -> 44568\n",
      "981410 -> 94188 -> 104189\n",
      "96799 -> 99769 -> 99769\n",
      "91983 -> 38929 -> 38919\n",
      "15359 -> 95351 -> 95351\n",
      "88587 -> 78588 -> 78588\n",
      "91862 -> 26829 -> 26819\n",
      "105491 -> 19459 -> 194510\n",
      "310922 -> 22993 -> 229103\n",
      "105916 -> 618610 -> 619510\n",
      "24837 -> 73842 -> 73842\n",
      "45477 -> 77454 -> 77454\n",
      "33491 -> 19433 -> 19433\n",
      "69646 -> 646106 -> 64696\n",
      "561063 -> 361065 -> 361065\n",
      "74163 -> 36147 -> 36147\n",
      "77433 -> 33487 -> 33477\n",
      "24994 -> 49942 -> 49942\n",
      "104549 -> 945410 -> 945410\n",
      "333310 -> 93333 -> 103333\n",
      "64668 -> 86646 -> 86646\n",
      "181074 -> 471082 -> 471081\n",
      "110751 -> 257102 -> 157101\n",
      "492810 -> 108294 -> 108294\n",
      "68593 -> 39586 -> 39586\n",
      "811056 -> 651018 -> 651018\n",
      "82682 -> 28628 -> 28628\n",
      "67815 -> 51876 -> 51876\n",
      "753101 -> 110357 -> 110357\n",
      "99731 -> 13799 -> 13799\n",
      "71911 -> 22826 -> 11917\n",
      "979510 -> 105979 -> 105979\n",
      "14252 -> 25241 -> 25241\n",
      "92442 -> 24429 -> 24429\n",
      "253410 -> 104352 -> 104352\n",
      "461910 -> 98164 -> 109164\n",
      "95673 -> 37659 -> 37659\n",
      "481022 -> 221084 -> 221084\n",
      "141042 -> 241051 -> 241041\n",
      "82741 -> 14728 -> 14728\n",
      "8710510 -> 1051078 -> 1051078\n",
      "14184 -> 48141 -> 48141\n",
      "82836 -> 63828 -> 63828\n",
      "93541 -> 14539 -> 14539\n",
      "71315 -> 51317 -> 51317\n",
      "32644 -> 44623 -> 44623\n",
      "44785 -> 58744 -> 58744\n",
      "10210810 -> 97939 -> 10810210\n",
      "64511 -> 11546 -> 11546\n",
      "516109 -> 99615 -> 910615\n",
      "79345 -> 543107 -> 54397\n",
      "55985 -> 58955 -> 58955\n",
      "771011 -> 11977 -> 111077\n",
      "95176 -> 67159 -> 67159\n",
      "16462 -> 26461 -> 26461\n",
      "44296 -> 69244 -> 69244\n",
      "3861010 -> 109683 -> 1010683\n",
      "10104510 -> 1054109 -> 10541010\n",
      "663109 -> 910366 -> 910366\n",
      "105951 -> 15969 -> 159510\n",
      "75879 -> 97857 -> 97857\n",
      "18782 -> 28782 -> 28781\n",
      "43435 -> 53434 -> 53434\n",
      "83998 -> 89938 -> 89938\n",
      "75157 -> 75057 -> 75157\n",
      "13754 -> 45730 -> 45731\n"
     ]
    }
   ],
   "source": [
    "# 100개의 예제 데이터 생성 (추론용)\n",
    "X_test, y_test = generate_data(100)\n",
    "\n",
    "# 데이터 reshape\n",
    "X_test = X_test.reshape(-1, 5, 1)\n",
    "\n",
    "# 예측 및 결과 출력\n",
    "print(\"입력 데이터 -> 모델 예측 -> 실제 출력\")\n",
    "for i in range(100):\n",
    "    input_data = X_test[i].reshape(1, 5, 1)\n",
    "    prediction = model.predict(input_data, verbose=0)\n",
    "    prediction = [int(round(x)) for x in prediction[0]]\n",
    "    input_str = ''.join(map(str, X_test[i].reshape(5)))\n",
    "    predicted_str = ''.join(map(str, prediction))\n",
    "    actual_str = ''.join(map(str, y_test[i]))\n",
    "    print(f\"{input_str} -> {predicted_str} -> {actual_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정답 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 2 1 0 9]\n",
      " [5 1 0 4 1]\n",
      " [4 0 2 4 2]\n",
      " [4 1 0 5 7]]\n",
      "[[9 0 1 2 8]\n",
      " [1 4 0 1 5]\n",
      " [2 4 2 0 4]\n",
      " [7 5 0 1 4]]\n",
      "[8 2 1 0 9]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │         \u001b[38;5;34m1,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m84,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,770</span> (338.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m86,770\u001b[0m (338.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,770</span> (338.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m86,770\u001b[0m (338.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding \n",
    "from tensorflow.keras.layers import Bidirectional \n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "#학습데이터 생성\n",
    "train_data = np.random.randint(0, 10, size=(10000, 5))\n",
    "label_data = np.flip(train_data, axis=1)\n",
    "print(train_data[:4])\n",
    "print(label_data[:4])\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#pad_sequence 전처리\n",
    "padded_train = pad_sequences(train_data, maxlen=5)\n",
    "print(padded_train[0])\n",
    "#모델링\n",
    "model = Sequential()\n",
    "model.add(Embedding(10,100, input_length=5))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True))) \n",
    "model.add(Dense(10,activation='softmax'))\n",
    "# 모델의 요약을 보기 위해 한 번 모델을 호출하여 초기화합니다\n",
    "model.build(input_shape=(None, 5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3485 - loss: 1.8580 - val_accuracy: 0.5405 - val_loss: 1.0724\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 1.0213 - val_accuracy: 0.5827 - val_loss: 0.9309\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5984 - loss: 0.8831 - val_accuracy: 0.6285 - val_loss: 0.8098\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6631 - loss: 0.7635 - val_accuracy: 0.7183 - val_loss: 0.6815\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.6340 - val_accuracy: 0.7935 - val_loss: 0.5568\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8357 - loss: 0.4875 - val_accuracy: 0.8678 - val_loss: 0.4032\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8902 - loss: 0.3591 - val_accuracy: 0.9230 - val_loss: 0.2752\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9359 - loss: 0.2436 - val_accuracy: 0.9552 - val_loss: 0.1905\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9636 - loss: 0.1619 - val_accuracy: 0.9735 - val_loss: 0.1284\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9819 - loss: 0.1063 - val_accuracy: 0.9864 - val_loss: 0.0839\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - accuracy: 0.9902 - loss: 0.0772\n",
      "loss : 0.0779, acc = 0.9898\n"
     ]
    }
   ],
   "source": [
    "#모델 컴파일\n",
    "model. compile(loss='sparse_categorical_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n",
    "#모델 실행\n",
    "history = model.fit(padded_train, label_data, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# loss? accuracy\n",
    "loss, acc = model. evaluate(padded_train, label_data)\n",
    "print(f\"loss : {loss:.4f}, acc = {acc:.4f}\")\n",
    "# 입력받아 예측하는 프로그램 작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "[5 4 3 2 1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "[5 0 7 8 9]\n",
      "종료합니다\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user = input(\"0~9 사이의 숫자 5개 또는 Quit: \")\n",
    "    if user == 'quit':\n",
    "        print(\"종료합니다\")\n",
    "        break\n",
    "    numbers = list(map(int, user.split(' ')))\n",
    "    array_numbers = np.array([numbers])\n",
    "    padded_numbers = pad_sequences(array_numbers, maxlen=5)\n",
    "    prediction = model.predict (padded_numbers)\n",
    "    print(np.argmax(prediction, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 숫자 -> 문자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Z' 'z' 'M' 'o' 'W' 'f' 'y' 'i' 'g' 'p']\n",
      " ['X' 'n' 'L' 'l' 'S' 'w' 'J' 'I' 'd' 'z']\n",
      " ['b' 'n' 'h' 'w' 'V' 'T' 'x' 'r' 'z' 'k']\n",
      " ['R' 'z' 'i' 'G' 'g' 'c' 'N' 'w' 'm' 'H']]\n",
      "[['p' 'g' 'i' 'y' 'f' 'W' 'o' 'M' 'z' 'Z']\n",
      " ['z' 'd' 'I' 'J' 'w' 'S' 'l' 'L' 'n' 'X']\n",
      " ['k' 'z' 'r' 'x' 'T' 'V' 'w' 'h' 'n' 'b']\n",
      " ['H' 'm' 'w' 'N' 'c' 'g' 'G' 'i' 'z' 'R']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding \n",
    "from tensorflow.keras.layers import Bidirectional \n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "cal_len = 10\n",
    "# 문자와 인덱스 매핑\n",
    "chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "num_classes = len(chars)\n",
    "\n",
    "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# 학습 데이터 생성\n",
    "train_data = np.random.choice(list(chars), size=(10000, cal_len))\n",
    "label_data = np.flip(train_data, axis=1)\n",
    "# 문자 데이터를 숫자 인덱스로 변환\n",
    "train_data_indices = np.array([[char_to_index[char] for char in seq] for seq in train_data])\n",
    "label_data_indices = np.array([[char_to_index[char] for char in seq] for seq in label_data])\n",
    "print (train_data[:4])\n",
    "print(label_data[ :4])\n",
    "# pad_sequence 전처리\n",
    "padded_train = pad_sequences (train_data_indices, maxlen=cal_len)\n",
    "padded_labels = pad_sequences (label_data_indices, maxlen=cal_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0620 - loss: 3.8030 - val_accuracy: 0.1836 - val_loss: 3.0548\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2028 - loss: 2.9045 - val_accuracy: 0.2192 - val_loss: 2.6895\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2243 - loss: 2.6027 - val_accuracy: 0.2258 - val_loss: 2.5260\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2342 - loss: 2.4553 - val_accuracy: 0.2295 - val_loss: 2.4320\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2364 - loss: 2.3635 - val_accuracy: 0.2320 - val_loss: 2.3755\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2416 - loss: 2.2901 - val_accuracy: 0.2340 - val_loss: 2.3209\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2417 - loss: 2.2353 - val_accuracy: 0.2352 - val_loss: 2.2754\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2457 - loss: 2.1874 - val_accuracy: 0.2365 - val_loss: 2.2454\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2503 - loss: 2.1452 - val_accuracy: 0.2380 - val_loss: 2.2163\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2532 - loss: 2.1158 - val_accuracy: 0.2419 - val_loss: 2.1913\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2552 - loss: 2.0801 - val_accuracy: 0.2413 - val_loss: 2.1665\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2558 - loss: 2.0558 - val_accuracy: 0.2411 - val_loss: 2.1517\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2628 - loss: 2.0241 - val_accuracy: 0.2449 - val_loss: 2.1287\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2636 - loss: 2.0065 - val_accuracy: 0.2463 - val_loss: 2.1127\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2695 - loss: 1.9798 - val_accuracy: 0.2480 - val_loss: 2.0913\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2731 - loss: 1.9574 - val_accuracy: 0.2484 - val_loss: 2.0716\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2759 - loss: 1.9364 - val_accuracy: 0.2499 - val_loss: 2.0572\n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2807 - loss: 1.9140 - val_accuracy: 0.2535 - val_loss: 2.0459\n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2834 - loss: 1.8988 - val_accuracy: 0.2569 - val_loss: 2.0343\n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2911 - loss: 1.8755 - val_accuracy: 0.2613 - val_loss: 2.0172\n",
      "Epoch 21/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2981 - loss: 1.8535 - val_accuracy: 0.2617 - val_loss: 2.0027\n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3012 - loss: 1.8344 - val_accuracy: 0.2663 - val_loss: 1.9876\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3083 - loss: 1.8136 - val_accuracy: 0.2739 - val_loss: 1.9745\n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.3158 - loss: 1.7950 - val_accuracy: 0.2752 - val_loss: 1.9581\n",
      "Epoch 25/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3246 - loss: 1.7722 - val_accuracy: 0.2824 - val_loss: 1.9479\n",
      "Epoch 26/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3304 - loss: 1.7519 - val_accuracy: 0.2889 - val_loss: 1.9131\n",
      "Epoch 27/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3405 - loss: 1.7276 - val_accuracy: 0.2953 - val_loss: 1.8953\n",
      "Epoch 28/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3507 - loss: 1.7035 - val_accuracy: 0.3036 - val_loss: 1.8849\n",
      "Epoch 29/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3587 - loss: 1.6788 - val_accuracy: 0.3171 - val_loss: 1.8531\n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3694 - loss: 1.6519 - val_accuracy: 0.3236 - val_loss: 1.8314\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3819 - loss: 1.6250 - val_accuracy: 0.3311 - val_loss: 1.7998\n",
      "Epoch 32/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3904 - loss: 1.5981 - val_accuracy: 0.3387 - val_loss: 1.7865\n",
      "Epoch 33/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4033 - loss: 1.5682 - val_accuracy: 0.3485 - val_loss: 1.7504\n",
      "Epoch 34/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4115 - loss: 1.5397 - val_accuracy: 0.3602 - val_loss: 1.7282\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4247 - loss: 1.5095 - val_accuracy: 0.3695 - val_loss: 1.7009\n",
      "Epoch 36/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4352 - loss: 1.4834 - val_accuracy: 0.3773 - val_loss: 1.6721\n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4469 - loss: 1.4461 - val_accuracy: 0.3873 - val_loss: 1.6362\n",
      "Epoch 38/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4550 - loss: 1.4229 - val_accuracy: 0.3981 - val_loss: 1.6110\n",
      "Epoch 39/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4677 - loss: 1.3961 - val_accuracy: 0.4107 - val_loss: 1.5796\n",
      "Epoch 40/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4797 - loss: 1.3617 - val_accuracy: 0.4128 - val_loss: 1.5587\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4935 - loss: 1.3293 - val_accuracy: 0.4254 - val_loss: 1.5329\n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5007 - loss: 1.3031 - val_accuracy: 0.4336 - val_loss: 1.5064\n",
      "Epoch 43/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5125 - loss: 1.2755 - val_accuracy: 0.4446 - val_loss: 1.4818\n",
      "Epoch 44/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5266 - loss: 1.2423 - val_accuracy: 0.4539 - val_loss: 1.4491\n",
      "Epoch 45/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5376 - loss: 1.2105 - val_accuracy: 0.4675 - val_loss: 1.4169\n",
      "Epoch 46/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5499 - loss: 1.1786 - val_accuracy: 0.4806 - val_loss: 1.3857\n",
      "Epoch 47/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5633 - loss: 1.1482 - val_accuracy: 0.4881 - val_loss: 1.3551\n",
      "Epoch 48/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5774 - loss: 1.1119 - val_accuracy: 0.4981 - val_loss: 1.3279\n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5901 - loss: 1.0834 - val_accuracy: 0.5145 - val_loss: 1.2930\n",
      "Epoch 50/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6051 - loss: 1.0432 - val_accuracy: 0.5310 - val_loss: 1.2566\n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6227 - loss: 1.0093 - val_accuracy: 0.5466 - val_loss: 1.2186\n",
      "Epoch 52/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6366 - loss: 0.9776 - val_accuracy: 0.5572 - val_loss: 1.1904\n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6507 - loss: 0.9466 - val_accuracy: 0.5718 - val_loss: 1.1589\n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6671 - loss: 0.9125 - val_accuracy: 0.5771 - val_loss: 1.1296\n",
      "Epoch 55/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6824 - loss: 0.8767 - val_accuracy: 0.5992 - val_loss: 1.0874\n",
      "Epoch 56/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6982 - loss: 0.8386 - val_accuracy: 0.6083 - val_loss: 1.0521\n",
      "Epoch 57/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7108 - loss: 0.8136 - val_accuracy: 0.6201 - val_loss: 1.0312\n",
      "Epoch 58/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7254 - loss: 0.7764 - val_accuracy: 0.6361 - val_loss: 0.9846\n",
      "Epoch 59/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7408 - loss: 0.7462 - val_accuracy: 0.6465 - val_loss: 0.9586\n",
      "Epoch 60/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7541 - loss: 0.7128 - val_accuracy: 0.6588 - val_loss: 0.9303\n",
      "Epoch 61/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7673 - loss: 0.6842 - val_accuracy: 0.6696 - val_loss: 0.9046\n",
      "Epoch 62/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7797 - loss: 0.6530 - val_accuracy: 0.6855 - val_loss: 0.8667\n",
      "Epoch 63/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7911 - loss: 0.6229 - val_accuracy: 0.6978 - val_loss: 0.8279\n",
      "Epoch 64/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8063 - loss: 0.5911 - val_accuracy: 0.7062 - val_loss: 0.8039\n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8159 - loss: 0.5651 - val_accuracy: 0.7190 - val_loss: 0.7700\n",
      "Epoch 66/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8269 - loss: 0.5422 - val_accuracy: 0.7285 - val_loss: 0.7481\n",
      "Epoch 67/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8357 - loss: 0.5166 - val_accuracy: 0.7412 - val_loss: 0.7219\n",
      "Epoch 68/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8517 - loss: 0.4844 - val_accuracy: 0.7494 - val_loss: 0.6992\n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8576 - loss: 0.4669 - val_accuracy: 0.7580 - val_loss: 0.6747\n",
      "Epoch 70/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8626 - loss: 0.4458 - val_accuracy: 0.7671 - val_loss: 0.6529\n",
      "Epoch 71/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8775 - loss: 0.4201 - val_accuracy: 0.7772 - val_loss: 0.6242\n",
      "Epoch 72/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8831 - loss: 0.4003 - val_accuracy: 0.7798 - val_loss: 0.6225\n",
      "Epoch 73/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.3867 - val_accuracy: 0.7883 - val_loss: 0.5962\n",
      "Epoch 74/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8954 - loss: 0.3681 - val_accuracy: 0.7986 - val_loss: 0.5758\n",
      "Epoch 75/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9047 - loss: 0.3455 - val_accuracy: 0.8056 - val_loss: 0.5526\n",
      "Epoch 76/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9080 - loss: 0.3327 - val_accuracy: 0.8096 - val_loss: 0.5438\n",
      "Epoch 77/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9143 - loss: 0.3188 - val_accuracy: 0.8192 - val_loss: 0.5252\n",
      "Epoch 78/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9220 - loss: 0.2984 - val_accuracy: 0.8224 - val_loss: 0.5095\n",
      "Epoch 79/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.2881 - val_accuracy: 0.8300 - val_loss: 0.4920\n",
      "Epoch 80/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9310 - loss: 0.2732 - val_accuracy: 0.8345 - val_loss: 0.4759\n",
      "Epoch 81/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9368 - loss: 0.2571 - val_accuracy: 0.8387 - val_loss: 0.4712\n",
      "Epoch 82/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.2474 - val_accuracy: 0.8472 - val_loss: 0.4441\n",
      "Epoch 83/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9440 - loss: 0.2351 - val_accuracy: 0.8480 - val_loss: 0.4453\n",
      "Epoch 84/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9470 - loss: 0.2270 - val_accuracy: 0.8548 - val_loss: 0.4229\n",
      "Epoch 85/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9517 - loss: 0.2150 - val_accuracy: 0.8545 - val_loss: 0.4243\n",
      "Epoch 86/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9541 - loss: 0.2050 - val_accuracy: 0.8576 - val_loss: 0.4154\n",
      "Epoch 87/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9540 - loss: 0.2014 - val_accuracy: 0.8638 - val_loss: 0.3962\n",
      "Epoch 88/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9612 - loss: 0.1854 - val_accuracy: 0.8701 - val_loss: 0.3875\n",
      "Epoch 89/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.1768 - val_accuracy: 0.8733 - val_loss: 0.3753\n",
      "Epoch 90/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9662 - loss: 0.1689 - val_accuracy: 0.8735 - val_loss: 0.3726\n",
      "Epoch 91/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.1572 - val_accuracy: 0.8779 - val_loss: 0.3572\n",
      "Epoch 92/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9727 - loss: 0.1518 - val_accuracy: 0.8750 - val_loss: 0.3645\n",
      "Epoch 93/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1498 - val_accuracy: 0.8807 - val_loss: 0.3479\n",
      "Epoch 94/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.1409 - val_accuracy: 0.8790 - val_loss: 0.3530\n",
      "Epoch 95/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9746 - loss: 0.1372 - val_accuracy: 0.8841 - val_loss: 0.3402\n",
      "Epoch 96/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.1273 - val_accuracy: 0.8885 - val_loss: 0.3298\n",
      "Epoch 97/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.1228 - val_accuracy: 0.8920 - val_loss: 0.3215\n",
      "Epoch 98/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9815 - loss: 0.1171 - val_accuracy: 0.8921 - val_loss: 0.3199\n",
      "Epoch 99/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9833 - loss: 0.1104 - val_accuracy: 0.8925 - val_loss: 0.3145\n",
      "Epoch 100/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9835 - loss: 0.1062 - val_accuracy: 0.8978 - val_loss: 0.3010\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.1032\n",
      " loss : 0.1388, acc = 0.9697\n"
     ]
    }
   ],
   "source": [
    "# 모델링\n",
    "model = Sequential()\n",
    "model. add (Embedding(num_classes, 100, input_length=cal_len))\n",
    "model.add (Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model. add (Dense(num_classes, activation='softmax'))\n",
    "model. summary\n",
    "# 모델 컴파일\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 모델 실행\n",
    "history = model. fit(padded_train, padded_labels, epochs=100, batch_size=32, validation_split=0.2)\n",
    "# losser accuracy\n",
    "loss, acc = model. evaluate(padded_train, padded_labels)\n",
    "print(f\" loss : {loss:.4f}, acc = {acc:.4f}\")\n",
    "# 랜덤한 영문 대소문자 5글자를 요소로 하는 20개 리스트 생성\n",
    "test_data = [\"\".join(np.random.choice(list(chars), cal_len)) for _ in range(20)]\n",
    "# 거꾸로 된 리스트 생성\n",
    "expected_outputs = [s[::-1] for s in test_data]\n",
    "# 모델 예측 및 정확도 계산\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "rect : DtgaQBBTdP\n",
      "예측된 출력: TdTBBgattD\n",
      "실제 출력: PdTBBQagtD\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "rect : gYQsXQabOm\n",
      "예측된 출력: OObaQXsYYg\n",
      "실제 출력: mObaQXsQYg\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "rect : ovNxZYUVCJ\n",
      "예측된 출력: JCVUYZxNvo\n",
      "실제 출력: JCVUYZxNvo\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : cJLnoPHcQQ\n",
      "예측된 출력: QQcHPonLJc\n",
      "실제 출력: QQcHPonLJc\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "rect : NBvNutiTUR\n",
      "예측된 출력: UUTituNNBN\n",
      "실제 출력: RUTituNvBN\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "rect : xmGxfsAHaX\n",
      "예측된 출력: XaHHsfxGmx\n",
      "실제 출력: XaHAsfxGmx\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : QQFWwnrahw\n",
      "예측된 출력: wharnwWFQQ\n",
      "실제 출력: wharnwWFQQ\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "rect : WwIlJvleLQ\n",
      "예측된 출력: QLelvJlIwW\n",
      "실제 출력: QLelvJlIwW\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "rect : iXsDDXiFda\n",
      "예측된 출력: adFiXDDDXL\n",
      "실제 출력: adFiXDDsXi\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "rect : FfQlyjDuMk\n",
      "예측된 출력: kuuDjylQfF\n",
      "실제 출력: kMuDjylQfF\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : LHGYqGnWba\n",
      "예측된 출력: abWGGqYGHL\n",
      "실제 출력: abWnGqYGHL\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "rect : pIVBXBhNkJ\n",
      "예측된 출력: JkNBBXBVIp\n",
      "실제 출력: JkNhBXBVIp\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "rect : wjvLHXxUMC\n",
      "예측된 출력: CMUxXHvvjw\n",
      "실제 출력: CMUxXHLvjw\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : aYNoXAbvvJ\n",
      "예측된 출력: JvvbAXoNYa\n",
      "실제 출력: JvvbAXoNYa\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "rect : CtttWPjxnz\n",
      "예측된 출력: znxjPWttta\n",
      "실제 출력: znxjPWtttC\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "rect : pqJHpfJADn\n",
      "예측된 출력: nDAJfpHJqp\n",
      "실제 출력: nDAJfpHJqp\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "rect : mYrburIZit\n",
      "예측된 출력: tiiIrubrYm\n",
      "실제 출력: tiZIrubrYm\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : COwQRqzMdW\n",
      "예측된 출력: WdMzqRQwOC\n",
      "실제 출력: WdMzqRQwOC\n",
      "정확여부: 맞음\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "rect : MaKzlVAlqr\n",
      "예측된 출력: RqlAVlzKKM\n",
      "실제 출력: rqlAVlzKaM\n",
      "정확여부: 틀림\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "rect : vPjEiiNrPz\n",
      "예측된 출력: zPrNiiEjPv\n",
      "실제 출력: zPrNiiEjPv\n",
      "정확여부: 맞음\n",
      "\n",
      "총 정확도: 40.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "for i, test_string in enumerate(test_data):\n",
    "    numbers = [char_to_index[char] for char in test_string]\n",
    "    array_numbers = np.array([numbers])\n",
    "    padded_numbers = pad_sequences(array_numbers, maxlen=cal_len)\n",
    "    prediction = model.predict(padded_numbers)\n",
    "    predicted_indices = np.argmax(prediction, axis=-1)[0]\n",
    "    predicted_chars = [index_to_char[idx] for idx in predicted_indices]\n",
    "    predicted_output = \"\".join(predicted_chars)\n",
    "    is_correct = predicted_output == expected_outputs[i]\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "    print(f\"rect : {test_string}\")\n",
    "    print(f\"예측된 출력: {predicted_output}\")\n",
    "    print(f\"실제 출력: {expected_outputs[i]}\")\n",
    "    print(f\"정확여부: {'맞음' if is_correct else '틀림'}\\n\")\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"총 정확도: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
