{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "#local os인 경우와 colap 인 경우 디렉토리 설정은 각각 다음과 같음\n",
    "imdb_dir ='aclImdb'\n",
    "#imdb_dir = '/content/drive/MyDrive/Colab Notebooks/dataset/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/train/neg\n",
      "aclImdb/train/pos\n",
      "Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n",
      "0\n",
      "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 라벨링\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    print(dir_name) # 확인용 출력\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(os.path.join(dir_name, fname), encoding='utf8') as f:\n",
    "                texts.append(f.read())\n",
    "            labels.append(0 if label_type == 'neg' else 1)\n",
    "\n",
    "print(texts[0]) # 부정의 첫 번째 문장\n",
    "print(labels[0]) # 부정의 첫 번째 Label\n",
    "print(texts[12500]) # 긍정의 첫 번째 문장\n",
    "print(labels[12500]) # 긍정의 첫 번째 Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582개의 고유한 토큰을 찾았습니다.\n",
      "데이터 텐서의 크기: (25000, 200)\n",
      "레이블 텐서의 크기: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras. layers import Embedding, LSTM, Dense, Bidirectional, Dropout \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# 데이터 전처리 설정\n",
    "maxlen = 200 # 200개 단어 이후는 버립니다\n",
    "training_samples = 10000\n",
    "validation_samples = 10000\n",
    "max_words = 10000 # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "\n",
    "# 토큰화 및 패딩\n",
    "tokenizer = Tokenizer (num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('데이터 텐서의 크기:'\n",
    ", data.shape)\n",
    "print('레이블 텐서의 크기:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 64, input_length=maxlen))\n",
    "model.add (Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add (Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 174ms/step - acc: 0.6211 - loss: 0.6229 - val_acc: 0.8189 - val_loss: 0.4055\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 241ms/step - acc: 0.8919 - loss: 0.2776 - val_acc: 0.8587 - val_loss: 0.3552\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 234ms/step - acc: 0.9497 - loss: 0.1476 - val_acc: 0.8430 - val_loss: 0.4589\n",
      "Epoch 4/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 227ms/step - acc: 0.9683 - loss: 0.0958 - val_acc: 0.8357 - val_loss: 0.5799\n",
      "Epoch 5/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 229ms/step - acc: 0.9815 - loss: 0.0612 - val_acc: 0.8377 - val_loss: 0.6214\n",
      "Epoch 6/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 235ms/step - acc: 0.9891 - loss: 0.0400 - val_acc: 0.8379 - val_loss: 0.5948\n",
      "Epoch 7/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 236ms/step - acc: 0.9912 - loss: 0.0328 - val_acc: 0.8389 - val_loss: 0.6673\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# 콜백 정의\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "# 모델 학습\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val),\n",
    "                    callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용하여 예측\n",
    "test_texts = [ #아래 문장은 강의 채널에 올려 놓음\n",
    "    \"I hated this movie. It was terrible and the acting was horrible.\",\n",
    "    \"This was the worst film I have ever seen. Not worth the time.\",\n",
    "    \"I loved this movie. It was fantastic and the acting was great.\",\n",
    "    \"This was the best film I have seen in a long time. Totally worth it.\",\n",
    "    \"I had high hopes for this movie, but it was a complete letdown . . . \",\n",
    "    \"This film was a disaster from start to finish. The dialogue  . . . \",\n",
    "    \"What an amazing film! The plot was deeply engaging, . . . \",\n",
    "    \"I was thoroughly impressed by this film. . . . \"\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('best_model.keras', custom_objects=None, compile=True)\n",
    "\n",
    "predictions = best_model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I hated this movie. ...\n",
      "Prediction: Negative (확률: 0.0540)\n",
      "Text: This was the worst f...\n",
      "Prediction: Negative (확률: 0.1854)\n",
      "Text: I loved this movie. ...\n",
      "Prediction: Positive (확률: 0.9273)\n",
      "Text: This was the best fi...\n",
      "Prediction: Positive (확률: 0.7730)\n",
      "Text: I had high hopes for...\n",
      "Prediction: Negative (확률: 0.2273)\n",
      "Text: This film was a disa...\n",
      "Prediction: Negative (확률: 0.3773)\n",
      "Text: What an amazing film...\n",
      "Prediction: Negative (확률: 0.3293)\n",
      "Text: I was thoroughly imp...\n",
      "Prediction: Positive (확률: 0.5323)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, test_text in enumerate(test_texts):\n",
    "    print(f\"Text: {test_text[:20]}...\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'} (확률: {predictions[i][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
